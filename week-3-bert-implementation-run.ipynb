{"cells":[{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install --upgrade transformers # only run this once per kernel session - dont want to overload kaggle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explanation of each library we use here:\n* numpy - numerical linear algebra library, makes it easy for us to do some array operations\n* transformers - huggingface's transformers NLP library, contains the BERT model + tokenizer that we will use\n* pickle - a library used for reading pickled files. Our data is pickled, so we need to use this library to open the data\n* tensorflow - the ML training library that we will use to train BERT and fine-tune it\n* re - regex python library, used in data cleaning\n* nltk - natural language toolkit, using in data cleaning"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport transformers as ppb # BERT Model\nimport pickle # decode pickled data\nimport tensorflow as tf\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import regexp_tokenize\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, load all the data into the program. We use the given training and testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = pd.read_pickle(\"../input/humor-detection/X_train.pickle\")\ntrain_y = pd.read_pickle(\"../input/humor-detection/y_train.pickle\")\ntest_x = pd.read_pickle(\"../input/humor-detection/X_test.pickle\")\ntest_y = pd.read_pickle(\"../input/humor-detection/y_test.pickle\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is the main model and tokenizer setup.\n\nTo tokenize our data, we use huggingface's BertTokenizer, which does some extra stuff on top of our own cleaning, like adding tokens like \\[CLS\\], and other necessary steps for BERT. We still have to do the same data cleaning that we had done previously, and so I copied over the steps from there into here.\n\nThe model is the TFBertForSequenceClassification model, which is basically a seqeunce classifier (like we want). This is better than previous models since the classification step and fine-tuning step are packed into one step, making it easier for us to use and work with."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"tokenizer = ppb.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = ppb.TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These data cleaning functions below perform the same functions as were done in the main data processing notebook:\n* ``lemmatize()`` - lemmatizes the sentence input ``s``, helps simplify model vocabulary\n* ``lower()`` - lowercases all the words in the sentence inputs ``s`` - is a remnant of a previous iteration, but I kept it around since it did no harm\n* ``clean()`` - a generalized cleaning function that does the two steps above + removes all numbers from ``data``, list of sentences passed in\n* ``tokenize()`` - tokenizes the list of sentences passed in ``text`` - this is what the ``BertTokenizer`` from the transformers library does. Returns an array of word vectors.\n* ``process()`` - a combination of cleaning and tokenizing, a function really created for our ease of use"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize(s):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    return \" \".join([wordnet_lemmatizer.lemmatize(w,'v') for w in s.split(\" \")])\ndef lower(s):\n    return s.lower()\ndef clean(data):\n    for item in data:\n        lemmatize(item)\n        lower(item)\n        re.sub(r'\\d+', '', item) # remove nums\n    return data\ndef tokenize(text):\n    tokenized = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")\n    return tokenized\ndef process(data):\n    cleaned = clean(data)\n    return tokenize(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_batch = process(train_x)\ntest_batch = process(test_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below we set up the TensorFlow model that we're going to use to classify and fine-tune BERT."},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 2e-5\nepochs = 10\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-8)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric1 = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmetric2 = tf.keras.metrics.Precision(name=\"precision\")\nmetric3 = tf.keras.metrics.Recall(name=\"recall\")\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=train_batch.input_ids, y=np.array(train_y), epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(x=test_batch.input_ids, y=np.array(test_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_pred = model.predict(x=test_batch.input_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred[0].shape)\ny_pred_bool = np.argmax(y_pred[0], axis=1)\nprint(np.array(test_y).shape)\nprint(y_pred_bool)\nprint(y_pred[0])\nprint(classification_report(test_y, y_pred_bool,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tk = tokenizer(\"When my son told me to stop impersonating a flamingo, I had to put my foot down.\", padding=True)\nout = model.predict(x=tk.input_ids)\nprint(np.argmax(out[0], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"Prediction\":y_pred_bool})\nsubmission.to_csv(\"predictions.csv\", index=True, index_label=\"Id\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}